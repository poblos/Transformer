# Implementation of a Transformer-Based Language Model
Implemented and trained a decoder-only Transformer model as a part of deep neural networks course. This project involved building the core architecture of a Transformer from scratch, including:
- Rotary Positional Encoding,
- self-attention mechanisms,
- layer normalisation,
- temperature and top_p.
